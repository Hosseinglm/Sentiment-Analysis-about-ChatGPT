{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "data = pd.read_csv('sentiment_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting seaborn\n",
      "  Using cached seaborn-0.12.2-py3-none-any.whl (293 kB)\n",
      "Requirement already satisfied: matplotlib!=3.6.1,>=3.1 in /Users/hossein.glm/Library/Python/3.9/lib/python/site-packages (from seaborn) (3.7.1)\n",
      "Requirement already satisfied: pandas>=0.25 in /Users/hossein.glm/Library/Python/3.9/lib/python/site-packages (from seaborn) (1.5.3)\n",
      "Requirement already satisfied: numpy!=1.24.0,>=1.17 in /Users/hossein.glm/Library/Python/3.9/lib/python/site-packages (from seaborn) (1.24.2)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /Users/hossein.glm/Library/Python/3.9/lib/python/site-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (2.8.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/hossein.glm/Library/Python/3.9/lib/python/site-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (21.3)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /Users/hossein.glm/Library/Python/3.9/lib/python/site-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (4.39.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /Users/hossein.glm/Library/Python/3.9/lib/python/site-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (1.0.7)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /Users/hossein.glm/Library/Python/3.9/lib/python/site-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (9.4.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /Users/hossein.glm/Library/Python/3.9/lib/python/site-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (0.11.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /Users/hossein.glm/Library/Python/3.9/lib/python/site-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (3.0.9)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /Users/hossein.glm/Library/Python/3.9/lib/python/site-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (1.4.4)\n",
      "Requirement already satisfied: importlib-resources>=3.2.0 in /Users/hossein.glm/Library/Python/3.9/lib/python/site-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (5.12.0)\n",
      "Requirement already satisfied: zipp>=3.1.0 in /Users/hossein.glm/Library/Python/3.9/lib/python/site-packages (from importlib-resources>=3.2.0->matplotlib!=3.6.1,>=3.1->seaborn) (3.11.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/hossein.glm/Library/Python/3.9/lib/python/site-packages (from pandas>=0.25->seaborn) (2022.7.1)\n",
      "Requirement already satisfied: six>=1.5 in /Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/site-packages (from python-dateutil>=2.7->matplotlib!=3.6.1,>=3.1->seaborn) (1.15.0)\n",
      "Installing collected packages: seaborn\n",
      "Successfully installed seaborn-0.12.2\n",
      "\u001b[33mWARNING: You are using pip version 21.2.4; however, version 23.0.1 is available.\n",
      "You should consider upgrading via the '/Library/Developer/CommandLineTools/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               tweet    labels\n",
      "0  ChatGPT: Optimizing Language Models for Dialog...   Neutral\n",
      "1  Try talking with ChatGPT, our new AI system wh...  Positive\n",
      "2  ChatGPT: Optimizing Language Models for Dialog...   Neutral\n",
      "3  THRILLED to share that ChatGPT, our new model ...  Positive\n",
      "4  As of 2 minutes ago, @OpenAI released their ne...  Negative\n"
     ]
    }
   ],
   "source": [
    "print(data.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          tweet    labels\n",
      "count    219294    219294\n",
      "unique   217622         3\n",
      "top     ChatGPT  Negative\n",
      "freq        122    107796\n"
     ]
    }
   ],
   "source": [
    "print(data.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 219294 entries, 0 to 219293\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count   Dtype \n",
      "---  ------  --------------   ----- \n",
      " 0   tweet   219294 non-null  object\n",
      " 1   labels  219294 non-null  object\n",
      "dtypes: object(2)\n",
      "memory usage: 3.3+ MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(data.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import re\n",
    "# nltk.download('stopwords')\n",
    "# from nltk.corpus import stopwords\n",
    "# from nltk.stem.porter import PorterStemmer\n",
    "# corpus = []\n",
    "# for i in range(0, 219294):\n",
    "#   #Remove any non-alphabetic characters from the tweet\n",
    "#   tweet = re.sub('[^a-zA-Z]', ' ', data['tweet'][i])\n",
    "#   #Convert the tweet to lowercase\n",
    "#   tweet = tweet.lower()\n",
    "#   #Split the tweet into a list of words\n",
    "#   tweet = tweet.split()\n",
    "#   ps = PorterStemmer()\n",
    "#   #Get the list of english stopwords from nltk\n",
    "#   all_stopwords = stopwords.words('english')\n",
    "#   #Remove the word ‘not’ from the stopwords list as it can be useful for sentiment analysis because sometimes \"not\" before some keywords create a positive meaning\n",
    "#   all_stopwords.remove('not')\n",
    "#   #Apply stemming to each word in the tweet and remove any stopwords, and also prepositions\n",
    "#   prepositions = ['to','the','a','as','in','is','of','i','it','be','this','you','that','be','on','with','for','about', 'above', 'across', 'after', 'along', 'among', 'around', 'at', 'before', 'behind', 'below', 'beneath', 'beside', 'between', 'beyond', 'by', 'down', 'during', 'except', 'inside', 'into', 'like', 'near', 'of', 'off', 'on', 'onto', 'out', 'outside', 'over', 'past', 'since', 'through', 'throughout', 'till', 'to', 'toward', 'under', 'underneath', 'until', 'up', 'upon', 'with', 'within', 'without']\n",
    "#   tweet = [ps.stem(word) for word in tweet if (not word in set(all_stopwords)) and (not word in set(prepositions))]\n",
    "#   #Join the words back into a single string\n",
    "#   tweet = ' '.join(tweet)\n",
    "#   #Append the processed tweet to the corpus list\n",
    "#   corpus.append(tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/hossein.glm/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "nltk.download('stopwords')\n",
    "\n",
    "ps = PorterStemmer()\n",
    "stop_words = set(stopwords.words('english')) - {'not'}\n",
    "prepositions = {'to', 'the', 'a', 'as', 'in', 'is', 'of', 'your','my','if','has','was','your','or','how','what','just','i','-','its','have', 'it', 'be', 'this', 'you', 'that', 'on','and', 'with', 'for','so','it\\'s','we','me','are','are','an','but','from', 'about', 'above', 'across', 'after', 'along', 'among', 'around', 'at', 'before', 'behind', 'below', 'beneath', 'beside', 'between', 'beyond', 'by', 'down', 'during', 'except', 'inside', 'into', 'like', 'near', 'off', 'onto', 'out', 'outside', 'over', 'past', 'since', 'through', 'throughout', 'till', 'toward', 'under', 'underneath', 'until', 'up', 'upon', 'within', 'without'}\n",
    "corpus = []\n",
    "for tweet in data['tweet']:\n",
    "    # Remove any non-alphabetic characters from the tweet\n",
    "    tweet = re.sub('[^a-zA-Z]', ' ', tweet)\n",
    "    # Convert the tweet to lowercase\n",
    "    tweet = tweet.lower()\n",
    "    # Split the tweet into a list of words\n",
    "    words = tweet.split()\n",
    "    # Apply stemming to each word in the tweet and remove any stopwords and prepositions\n",
    "    words = [ps.stem(word) for word in words if word not in stop_words and word not in prepositions]\n",
    "    # Join the words back into a single string\n",
    "    processed_tweet = ' '.join(words)\n",
    "    # Append the processed tweet to the corpus list\n",
    "    corpus.append(processed_tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "#Create a count vectorizer object with a maximum of 20000 features (words)\n",
    "cv = CountVectorizer(max_features = 20000)\n",
    "#Fit and transform the corpus into a sparse matrix of word counts\n",
    "X = cv.fit_transform(corpus).toarray()\n",
    "#Get the target variable (sentiment) from the data as a numpy array\n",
    "y = data.iloc[:, -1].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=101)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a multinomial naive bayes classifier object and Fit the classifier on the training data\n",
    "\n",
    "# from sklearn.naive_bayes import MultinomialNB\n",
    "# classifier = MultinomialNB()\n",
    "# classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCanceled future for execute_request message before replies were done"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Define the classifier\n",
    "classifier = MultinomialNB()\n",
    "\n",
    "# Define the pipeline for feature engineering and classification\n",
    "pipeline = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer()),\n",
    "    ('classifier', classifier)\n",
    "])\n",
    "\n",
    "# Define the hyperparameter search space\n",
    "params = {\n",
    "    'tfidf__max_df': [0.5, 0.75, 1.0],\n",
    "    'tfidf__min_df': [1, 2, 3],\n",
    "    'tfidf__ngram_range': [(1,1), (1,2), (2,2)],\n",
    "    'classifier__alpha': [0.1, 0.5, 1.0],\n",
    "    'classifier__fit_prior': [True, False]\n",
    "}\n",
    "\n",
    "# Perform grid search to find the best hyperparameters\n",
    "grid_search = GridSearchCV(pipeline, params, cv=5, n_jobs=-1)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict the sentiment labels for the test data using the classifier\n",
    "y_pred = classifier.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(cm)\n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorize text using TF-IDF\n",
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(data['tweet'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate classification report and confusion matrix\n",
    "print(classification_report(y_test, y_pred))\n",
    "confusion_matrix = confusion_matrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution of sentiment labels\n",
    "sns.countplot(x='labels', data=data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution of tweet lengths\n",
    "data['tweet_length'] = data['tweet'].apply(len)\n",
    "sns.histplot(x='tweet_length', data=data, bins=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top 20 most common words in positive tweets\n",
    "positive_tweets = data[data['labels'] == 'Positive']\n",
    "positive_word_freq = pd.Series(' '.join(positive_tweets['tweet']).lower().split()).value_counts().drop(prepositions)\n",
    "top_positive_words = positive_word_freq.head(20)\n",
    "sns.barplot(x=top_positive_words.values, y=top_positive_words.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top 20 most common words in negative tweets\n",
    "negative_tweets = data[data['labels'] == 'Negative']\n",
    "negative_word_freq = pd.Series(' '.join(negative_tweets['tweet']).lower().split()).value_counts().drop(prepositions)\n",
    "top_negative_words = negative_word_freq.head(20)\n",
    "sns.barplot(x=top_negative_words.values, y=top_negative_words.index)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
